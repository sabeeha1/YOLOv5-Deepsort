{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"M8JpUo_ulzcF"},"source":["[1]*10**10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Install latest YOLOv5 version and dependencies. Store the working directory"],"metadata":{"id":"O9vKiAV98zA9"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZfHCsusVxIc","executionInfo":{"status":"ok","timestamp":1639560036046,"user_tz":-780,"elapsed":4523,"user":{"displayName":"Shazia Mehtab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09451431503292600876"}},"outputId":"0cc06804-93a6-4fa5-e8da-22bc7ac013b3"},"source":["%cd Base/Directory\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","!pip install -qr yolov5/requirements.txt  # install dependencies (ignore errors)\n","%cd yolov5\n","\n","import torch\n","from IPython.display import Image, clear_output  # to display images\n","\n","\n","clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.10.0+cu111 CPU\n"]}]},{"cell_type":"markdown","source":["# Loading Training, testing and Validation datasets. Keep the training data in yolo format, i.e., (centre_x,centre_y,width,height) in normalized format"],"metadata":{"id":"5zcQKfE48Vb0"}},{"cell_type":"code","metadata":{"id":"y3j0fW1XWboK"},"source":["# !apt install unzip\n","from zipfile import ZipFile\n","zf = ZipFile('Path/to/train.zip', 'r')\n","zf.extractall('Base/Folder')\n","zf = ZipFile('Path/to/valid.zip', 'r')\n","zf.extractall('Base/Folder')\n","zf = ZipFile('Base/test_video.zip', 'r')\n","zf.extractall('Base/Folder')\n","zf.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uu74hUc-Wdp1","executionInfo":{"status":"ok","timestamp":1638586315068,"user_tz":-780,"elapsed":510475,"user":{"displayName":"sameena mehtab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01368863602932025503"}},"outputId":"ce3c5493-06cd-4883-c1a0-313d2f8a9bc5"},"source":["# Train YOLOv5s on coco128 for 3 epochs /content/drive/MyDrive/YOLO/yolov5/datasets/MOT/valid.txt\n","!python train.py --img 256 --batch 16 --epochs 300 --data data.yaml --weights yolov5s.pt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YOLO/yolov5\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=farah.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=300, batch_size=16, imgsz=256, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mCommand 'git fetch && git config --get remote.origin.url' timed out after 5 seconds\n","YOLOv5 ðŸš€ 2021-12-3 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model Summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.8 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","Scaled weight_decay = 0.0005\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'datasets/Farah/train' images and labels...800 found, 0 missing, 0 empty, 0 corrupted: 100% 800/800 [00:03<00:00, 209.27it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: datasets/Farah/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'datasets/Farah/valid' images and labels...200 found, 0 missing, 0 empty, 0 corrupted: 100% 200/200 [00:01<00:00, 142.53it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: datasets/Farah/valid.cache\n","Plotting labels to runs/train/exp21/labels.jpg... \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.00 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n","Image sizes 256 train, 256 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp21\u001b[0m\n","Starting training for 300 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     0/299    0.575G    0.1245   0.02993         0       127       256: 100% 50/50 [00:20<00:00,  2.41it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.76it/s]\n","                 all        200       1082     0.0071     0.0915     0.0039   0.000786\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     1/299    0.663G    0.1052   0.03235         0       125       256: 100% 50/50 [00:18<00:00,  2.70it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.67it/s]\n","                 all        200       1082      0.488      0.531      0.438     0.0858\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     2/299    0.663G   0.07262   0.02616         0       134       256: 100% 50/50 [00:18<00:00,  2.70it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.71it/s]\n","                 all        200       1082      0.535      0.699      0.588      0.168\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     3/299    0.663G   0.07034   0.02512         0       103       256: 100% 50/50 [00:18<00:00,  2.74it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.63it/s]\n","                 all        200       1082      0.781       0.82      0.863      0.364\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     4/299    0.663G   0.06504   0.02398         0       108       256: 100% 50/50 [00:17<00:00,  2.84it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.83it/s]\n","                 all        200       1082      0.782      0.823      0.864      0.425\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     5/299    0.663G   0.05937   0.02431         0       141       256: 100% 50/50 [00:17<00:00,  2.85it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.81it/s]\n","                 all        200       1082      0.771      0.776      0.834      0.415\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     6/299    0.663G   0.05733   0.02431         0       132       256: 100% 50/50 [00:18<00:00,  2.76it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.78it/s]\n","                 all        200       1082      0.757      0.881      0.852      0.386\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     7/299    0.663G   0.05455   0.02281         0       118       256: 100% 50/50 [00:18<00:00,  2.75it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.13it/s]\n","                 all        200       1082      0.967      0.982       0.99      0.569\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     8/299    0.663G   0.05213   0.02301         0       124       256: 100% 50/50 [00:18<00:00,  2.67it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.81it/s]\n","                 all        200       1082      0.956      0.988      0.986      0.617\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     9/299    0.663G   0.04925   0.02219         0       130       256: 100% 50/50 [00:18<00:00,  2.65it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.06it/s]\n","                 all        200       1082      0.978       0.99      0.994      0.594\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    10/299    0.663G   0.05372   0.02251         0       139       256: 100% 50/50 [00:17<00:00,  2.86it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.05it/s]\n","                 all        200       1082      0.826      0.933      0.901      0.448\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    11/299    0.663G   0.05132   0.02219         0       137       256: 100% 50/50 [00:17<00:00,  2.86it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.01it/s]\n","                 all        200       1082      0.918      0.917      0.963      0.531\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    12/299    0.663G   0.04976   0.02166         0       103       256: 100% 50/50 [00:17<00:00,  2.88it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.00it/s]\n","                 all        200       1082      0.965      0.982       0.99      0.615\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    13/299    0.663G     0.046    0.0212         0        98       256: 100% 50/50 [00:17<00:00,  2.80it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.09it/s]\n","                 all        200       1082      0.978      0.995      0.994      0.636\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    14/299    0.663G   0.04645    0.0206         0       136       256: 100% 50/50 [00:17<00:00,  2.78it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.94it/s]\n","                 all        200       1082      0.965          1      0.988      0.549\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    15/299    0.665G   0.04351   0.02127         0       120       256: 100% 50/50 [00:17<00:00,  2.85it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.24it/s]\n","                 all        200       1082      0.984      0.991      0.994      0.601\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    16/299    0.665G   0.04396   0.02056         0       100       256: 100% 50/50 [00:17<00:00,  2.89it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.12it/s]\n","                 all        200       1082       0.98      0.993      0.991      0.613\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    17/299    0.665G     0.049   0.02084         0       110       256: 100% 50/50 [00:17<00:00,  2.92it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.06it/s]\n","                 all        200       1082      0.943      0.981      0.984      0.598\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    18/299    0.665G   0.04436   0.02038         0       113       256: 100% 50/50 [00:17<00:00,  2.92it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  2.99it/s]\n","                 all        200       1082      0.983      0.989      0.992        0.6\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    19/299    0.665G   0.04467    0.0211         0       111       256: 100% 50/50 [00:17<00:00,  2.90it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.10it/s]\n","                 all        200       1082      0.939      0.998      0.965       0.53\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    20/299    0.665G   0.04258   0.01965         0       114       256: 100% 50/50 [00:17<00:00,  2.92it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.18it/s]\n","                 all        200       1082      0.983      0.994      0.994      0.634\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    21/299    0.665G   0.03963   0.01956         0       120       256: 100% 50/50 [00:17<00:00,  2.88it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:02<00:00,  3.18it/s]\n","                 all        200       1082      0.972      0.994      0.991       0.65\n","Traceback (most recent call last):\n","  File \"train.py\", line 626, in <module>\n","    main(opt)\n","  File \"train.py\", line 523, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"train.py\", line 387, in train\n","    torch.save(ckpt, last)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 379, in save\n","    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 484, in _save\n","    pickler.dump(obj)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 461, in persistent_id\n","    def persistent_id(obj):\n","KeyboardInterrupt\n"]}]},{"cell_type":"markdown","source":["# Making performance curves using TensorBoard"],"metadata":{"id":"Z6xAPbFs9yHZ"}},{"cell_type":"code","metadata":{"id":"zfngRKNXatOb"},"source":["%reload_ext tensorboard\n","%tensorboard --logdir runs "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" %cd path/to/YOLOv5/folder\n","!python detect.py --weights runs/train/exp0/weights/best.pt --img 640 --conf 0.1 --source base/directory/Test_video.mp4                      "],"metadata":{"id":"bquyZIi9dr1B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cloning Deepsort"],"metadata":{"id":"udO10NVn-FJM"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4s2pXKYcZayD","executionInfo":{"status":"ok","timestamp":1639644361613,"user_tz":-780,"elapsed":315,"user":{"displayName":"Shazia Mehtab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09451431503292600876"}},"outputId":"71a51d04-a634-41df-84f3-0f6126d370e2"},"source":["!git clone --recurse-submodules https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YOLO\n","fatal: destination path 'Yolov5_DeepSort_Pytorch' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","metadata":{"id":"nO8kXfdRbe23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639644370933,"user_tz":-780,"elapsed":5138,"user":{"displayName":"Shazia Mehtab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09451431503292600876"}},"outputId":"e552dbaa-8816-429d-c5b0-8e5fb53f8af2"},"source":["%cd Base/Folder/Yolov5_DeepSort_Pytorch\n","!pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YOLO/Yolov5_DeepSort_Pytorch\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.19.5)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.1.2.30)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n","Collecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.23.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.4.1)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.11.1+cu111)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.62.3)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.1.5)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.11.2)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.9)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 12)) (3.10.0.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 18)) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.15.0)\n","Installing collected packages: PyYAML\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0\n"]}]},{"cell_type":"code","metadata":{"id":"EJmvQ3Mjbsy7"},"source":["!python track.py --source path/to/test/frames --yolo_weights path/to/best.pt --classes 0 --classes 0 --save-txt   # video --save-txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"lFu_s5cHz5eq","executionInfo":{"status":"ok","timestamp":1639644313429,"user_tz":-780,"elapsed":29004,"user":{"displayName":"Shazia Mehtab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09451431503292600876"}},"outputId":"be92cef6-78db-4900-9827-b6b65b1fa23f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"DJJS7lOCzNIi"},"source":[" '''Here to save results in text form, I have made changes in the visualize_boxes_and_labels_on_image_array function\n","  by putting class and box value in the print statement and saving them directly form output screen.'''\n","import numpy as np\n","from PIL import Image\n","import os\n","import glob\n","import cv2\n","import matplotlib.pyplot as plt\n","import warnings\n","from object_detection.utils import vis4 #changed version of visualization_utils, where print statement is added at 1241line number\n","from object_detection.utils import visualization_utils\n","import tensorflow.python.ops.numpy_ops.np_config\n","\n","warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n","IMAGE_PATHS = 'path/to/test/frames/*.jpg'\n","\n","def load_image_into_numpy_array(path):\n","    \"\"\"Load an image from file into a numpy array.\n","\n","    Puts image into numpy array to feed into tensorflow graph.\n","    Note that by convention we put it into a numpy array with shape\n","    (height, width, channels), where channels=3 for RGB.\n","\n","    Args:\n","      path: the file path to the image\n","\n","    Returns:\n","      uint8 numpy array with shape (img_height, img_width, 3)\n","    \"\"\"\n","    return np.array(Image.open(path))\n","\n","\n","for image_path in glob.glob(IMAGE_PATHS):\n","    filename = os.path.basename(image_path)\n","\n","    print('Running inference for {}...\\n'.format(image_path), end='')\n","\n","    image_np = load_image_into_numpy_array(image_path)\n","\n","    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n","    input_tensor = tf.convert_to_tensor(image_np)\n","    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    # input_tensor = np.expand_dims(image_np, 0)\n","    detections = detect_fn(input_tensor)\n","    \n","    # All outputs are batches tensors.\n","    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","    # We're only interested in the first num_detections.\n","    num_detections = int(detections.pop('num_detections'))\n","    detections = {key: value[0, :num_detections].numpy()\n","                   for key, value in detections.items()}\n","    detections['num_detections'] = num_detections\n","    # print(detections['num_detections'])\n","    # detection_classes should be ints.\n","    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","    image_np_with_detections = image_np.copy()\n","    boxes = detections['detection_boxes']\n","    lboxes = len(detections['detection_boxes'])\n","    lclasses = len(detections['detection_classes'])\n","    #print(bgoxes,lboxes,lclasses)\n","    vis4.visualize_boxes_and_labels_on_image_array(\n","          image_np_with_detections,\n","          detections['detection_boxes'],\n","          detections['detection_classes'],\n","          detections['detection_scores'],\n","          category_index,\n","          use_normalized_coordinates=True,\n","          max_boxes_to_draw=200,\n","          min_score_thresh=.30,\n","          agnostic_mode=False)\n","    \n","    plt.figure()\n","    plt.imshow(image_np_with_detections)\n","    plt.show()\n","    data = Image.fromarray(image_np_with_detections)\n","    data.save('<Path to out images files>'+filename)\n","    print('Done')\n","    \n","    \n","    plt.figure(figsize=IMAGE_SIZE)\n","    plt.imshow(image_np)\n","\n","   \n"],"execution_count":null,"outputs":[]}]}